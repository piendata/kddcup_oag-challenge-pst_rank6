
## Prerequisites
- Linux
- Python 3.10
- CUDA 12.0
- NVIDIA A100

### Installation

Clone this repo.

```bash
git clone 
cd kddcup_oag-challenge-pst_rank6
```

Please install dependencies by

```bash
pip install -r requirements.txt
```

## PST Dataset
The dataset can be downloaded from [BaiduPan](https://pan.baidu.com/s/1I_HZXBx7U0UsRHJL5JJagw?pwd=bft3) with password bft3, [Aliyun](https://open-data-set.oss-cn-beijing.aliyuncs.com/oag-benchmark/kddcup-2024/PST/PST.zip) or [DropBox](https://www.dropbox.com/scl/fi/namx1n55xzqil4zbkd5sv/PST.zip?rlkey=impcbm2acqmqhurv2oj0xxysx&dl=1).
The paper XML files are generated by [Grobid](https://grobid.readthedocs.io/en/latest/Introduction/) APIs from paper pdfs.
And please download the DBLP-Citation-network V16 from [DBLP](https://open.aminer.cn/open/article?id=655db2202ab17a072284bc0c) and version OAG 3.1 from [OAG](https://open.aminer.cn/open/article?id=5965cf249ed5db41ed4f52bf), extract them, and place them in the data folder.


## Directory structure
```bash
--kddcup_oag-challenge-pst_rank6
	--script
		--...(some files)
	--data
    	--PST
    		--...(some files)
    		--paper-xml(load competition dataset)
```

## method
### 1_data_manipulation.ipynb
This notebook uses XML files and data from DBLP and OAG to extract the following information about academic papers and their references:

- Title
- Abstract
- Text information near the references (context)

The XML files do not provide reference IDs, only titles and author information are available. To obtain further details such as abstracts of the references, it is necessary to correctly match records with the DBLP and OAG data.
The processed data is then combined with the given train, validation, and test datasets, and undergoes preprocessing such as normalization.
During the process, some features are created. The citation number and citation count extracted from each reference in the XML files have been significant features in my model.

### 2_text_embedding.ipynb
Text embeddings are performed on the titles of papers and their references using `multilingual-e5-large`, and cosine similarity is calculated. Additionally, these embedding vectors are reduced to two dimensions using UMAP.

Similar processing is also applied to abstracts and contexts, particularly retrieving the text before and after citation points and calculating the cosine similarity of these pairs. These generated similarities have served as important features in my model.

### 3_network_processing.ipynb
Features related to citation counts and page information for each author were created using data from sources like DBLP. Additionally, a network of references was built, and after creating embeddings for nodes using node2vec to capture the network relationships of papers from sources like DBLP, these embeddings were dimensionally reduced using UMAP and used as features.

### 4_createMLdataset_and_train.ipynb
The features generated above were used to conduct machine learning training and inference. Training was performed using LightGBM and CatBoost, with parameter optimization conducted using Optuna. Due to the very limited number of positive examples available, BorderlineSMOTE was used to perform oversampling to increase the data volume. The final output was produced by ensembling the prediction results of LightGBM and CatBoost.

### 5_inference.ipynb
Load the model with the highest score obtained during the training process and make predictions. Also, generate a submission file.

## note
The "make_submission_file.ipynb" notebook formats the output in the submission file format and includes titles. It is used for comparing prediction results, and the generated file is stored as "valid_submission_test.json" in the data folder.

If you have any questions, please contact me. Email:piendata@gmail.com