{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67352cad-0da3-4598-8d70-4e32825f8cb9",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89463751-4946-414d-9882-51a5ef58826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import csv\n",
    "import random\n",
    "from lxml import etree\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from collections import defaultdict as dd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import settings\n",
    "\n",
    "import logging\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import torch_cluster\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import pandas as pd\n",
    "import joblib  \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')  # include timestamp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "dir_path = '../data/PST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c0af7a-74d0-4ecd-abee-a68df3b5000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_df = pd.read_pickle(dir_path + 'OAG_DBLP-Citation-network.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abf4275-e384-43e3-b1fe-336e914d238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_analysis = pd.read_pickle(dir_path + 'dataset_v2.pkl')\n",
    "\n",
    "valid_data = pd.read_json(dir_path + 'paper_source_trace_valid_wo_ans.json')\n",
    "train_data = pd.read_json(dir_path + 'paper_source_trace_train_ans.json')\n",
    "test_data = pd.read_json(dir_path + 'paper_source_trace_test_wo_ans.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc3f3d-9c71-4564-aee8-ffdaaa1ccc14",
   "metadata": {},
   "source": [
    "### basic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1101514c-6357-46d5-8101-35fa9b717a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6999535/6999535 [50:39<00:00, 2303.17it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id                 author_id   author_type\n",
      "0  5390877920f70186a0d2cb7c  53f42ed4dabfaedd74d498b4  first_author\n",
      "1  5390877920f70186a0d2cc08  53f45948dabfaee02ad63b9c  first_author\n",
      "2  5390877920f70186a0d2cc08  560bdcad45cedb3397402156   last_author\n",
      "3  5390877920f70186a0d2ce7f  56055d5945cedb33965f78ad  first_author\n",
      "4  5390877920f70186a0d2ce7f  548a62d3dabfae8a11fb49e2   last_author\n"
     ]
    }
   ],
   "source": [
    "# Convert 'n_citation' column to numeric, coercing errors to NaN\n",
    "dblp_df['n_citation'] = pd.to_numeric(dblp_df['n_citation'], errors='coerce')\n",
    "\n",
    "# Calculate the average citation count per venue to estimate venue impact\n",
    "venue_impact = dblp_df.groupby('venue')['n_citation'].mean().sort_values(ascending=False)\n",
    "\n",
    "(venue_impact.reset_index()).to_pickle(dir_path + 'venue_impact_v2.pkl')\n",
    "\n",
    "target_paper_ids = list(set(list(valid_data._id)+\n",
    "                            list(train_data._id)\n",
    "                            +list(test_data._id)+\n",
    "                           list(xml_analysis.loc[xml_analysis['target'] == 1]._id)))\n",
    "\n",
    "dblp_df['references'] = dblp_df['references'].apply(lambda x: x if isinstance(x, str) else str(x))\n",
    "\n",
    "# Create a dictionary to hold the count of how many times each paper is cited\n",
    "citation_counts = dblp_df['references'].explode().value_counts().to_dict()\n",
    "# Map the citation counts to the original dataframe using the 'id' column\n",
    "dblp_df['in_citation_count'] = dblp_df['id'].map(citation_counts).fillna(0)\n",
    "\n",
    "dblp_df['page_start'] = pd.to_numeric(dblp_df['page_start'], errors='coerce')\n",
    "dblp_df['page_end'] = pd.to_numeric(dblp_df['page_end'], errors='coerce')\n",
    "\n",
    "# ページ数を計算\n",
    "dblp_df['page_count'] = dblp_df['page_end'] - dblp_df['page_start'] + 1\n",
    "\n",
    "# データを確認\n",
    "dblp_df[['id', 'title', 'page_start', 'page_end', 'page_count']].head()\n",
    "\n",
    "\n",
    "dblp_df[['id','in_citation_count','page_count']].to_pickle(dir_path + 'dblp_feature_v2.pkl')\n",
    "\n",
    "df = dblp_df[['id','authors','n_citation']].copy()\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "\n",
    "def expand_authors(row):\n",
    "    authors = row['authors']\n",
    "    if isinstance(authors, str):\n",
    "        authors = eval(authors)\n",
    "    \n",
    "    if authors:\n",
    "        first_author = authors[0]\n",
    "        last_author = authors[-1]\n",
    "        authors_extracted = []\n",
    "        if first_author:\n",
    "            authors_extracted.append({'id': row['id'], 'author_id': first_author.get('id', ''), 'author_type': 'first_author'})\n",
    "        if last_author and last_author != first_author:\n",
    "            authors_extracted.append({'id': row['id'], 'author_id': last_author.get('id', ''), 'author_type': 'last_author'})\n",
    "        return authors_extracted\n",
    "    return []\n",
    "\n",
    "expanded_authors = pd.concat([pd.DataFrame(expand_authors(row)) for index, row in tqdm(df.iterrows(), total=df.shape[0])], ignore_index=True)\n",
    "\n",
    "print(expanded_authors.head())\n",
    "\n",
    "\n",
    "df = df.merge(expanded_authors, on = 'id', how = 'left')\n",
    "\n",
    "temp = df[['author_id','n_citation']].groupby(['author_id']).sum().reset_index()\n",
    "\n",
    "temp2 = temp.merge(df[['id','author_id']], how = 'left',on = 'author_id')\n",
    "temp2 = temp2.drop_duplicates()\n",
    "\n",
    "temp2 = temp2[['id','n_citation']].groupby(['id']).sum().reset_index()\n",
    "\n",
    "temp2.to_pickle(dir_path + 'author_citation_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bed07-e4dc-4742-8e34-d353a7930de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c4e3b59-c6f7-4669-8068-9d8514ef66f0",
   "metadata": {},
   "source": [
    "### node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7fbf987-7ff2-4a13-be1e-9b9483ad66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_df = pd.read_pickle(dir_path + 'OAG_DBLP-Citation-network.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44451e26-7ad3-4f10-bcd5-0fc31e213afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_filtered_network = nx.DiGraph()\n",
    "\n",
    "for index, row in tqdm(dblp_df.iterrows(), total=dblp_df.shape[0], desc=\"Building Network\"):\n",
    "    references = row['references']\n",
    "    if isinstance(references, str):\n",
    "        references = eval(references)\n",
    "    \n",
    "    if references: \n",
    "        efficient_filtered_network.add_node(row['id'])\n",
    "        for ref_id in references:\n",
    "            efficient_filtered_network.add_edge(row['id'], ref_id)\n",
    "            if ref_id not in efficient_filtered_network:\n",
    "                efficient_filtered_network.add_node(ref_id)\n",
    "\n",
    "print(\"Number of nodes:\", efficient_filtered_network.number_of_nodes())\n",
    "print(\"Number of edges:\", efficient_filtered_network.number_of_edges())\n",
    "print(\"Sample nodes:\", list(efficient_filtered_network.nodes())[:5])\n",
    "print(\"Sample edges:\", list(efficient_filtered_network.edges())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cda9e2-3133-4e4b-bfb5-13c79b2bc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_graph = from_networkx(efficient_filtered_network)\n",
    "\n",
    "node_ids = list(efficient_filtered_network.nodes())\n",
    "\n",
    "t = node_ids\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Node2Vec(torch_graph.edge_index, embedding_dim=64, walk_length=10,\n",
    "                 context_size=5, walks_per_node=5, num_negative_samples=5,\n",
    "                 p=1, q=1.1, sparse=True)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()} GPUs が利用可能です。DataParallelを使用します。\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "loader = model.module.loader(batch_size=128, shuffle=True, num_workers=4) if isinstance(model, torch.nn.DataParallel) else model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model.module.loss(pos_rw.to(device), neg_rw.to(device)) if isinstance(model, torch.nn.DataParallel) else model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(7):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    embeddings = model()\n",
    "\n",
    "print(embeddings.shape)  # Checking the shape of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebdd11-4428-4c6a-8342-21c131a85f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_size = 2000000  # サンプルサイズは状況に応じて調整\n",
    "indices = np.random.choice(embeddings.shape[0], sample_size, replace=False)\n",
    "sampled_embeddings = embeddings[indices]\n",
    "sampled_embeddings_cpu = sampled_embeddings.detach().cpu().numpy()  # ここを修正\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=30, n_components=2, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "umap_model.fit(sampled_embeddings_cpu) \n",
    "\n",
    "sampled_embeddings_cpu = embeddings.detach().cpu().numpy()  \n",
    "\n",
    "sampled_embedding_reduced = umap_model.transform(sampled_embeddings_cpu)\n",
    "\n",
    "df_full = pd.DataFrame(sampled_embedding_reduced, columns=['UMAP1', 'UMAP2'])\n",
    "\n",
    "df_full['node_id'] = t\n",
    "df_full[['UMAP1','UMAP2','node_id']].to_csv(dir_path + 'node2vec_umap_v3.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
