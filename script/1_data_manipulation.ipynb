{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc3804-503d-4022-a1a4-24ba46b5c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import csv\n",
    "import random\n",
    "from lxml import etree\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from collections import defaultdict as dd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "import glob\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')  # include timestamp\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "import settings\n",
    "dir_path = settings.DIR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7b79e-8f51-4be3-8d18-92647edb3e8f",
   "metadata": {},
   "source": [
    "### Processing Open Academic Papers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fbf55-a190-4dfc-812e-15de6096c9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paper_dict_open = {}\n",
    "dblp_fname = \"DBLP-Citation-network-V15.1.json\"\n",
    "with open(join(dir_path, dblp_fname), \"r\", encoding=\"utf-8\") as myFile:\n",
    "    for i, line in enumerate(myFile):\n",
    "        if len(line) <= 2:\n",
    "            continue\n",
    "        if i % 10000 == 0: \n",
    "            logger.info(\"reading papers %d\", i)\n",
    "        paper_tmp = json.loads(line.strip())\n",
    "        paper_dict_open[paper_tmp[\"id\"]] = paper_tmp\n",
    "\n",
    "import pandas as pd\n",
    "temp = pd.DataFrame(paper_dict_open)\n",
    "temp = temp.T.reset_index(drop = True)\n",
    "\n",
    "dblp_df = temp.copy()\n",
    "\n",
    "paper_dict_open = {}\n",
    "dblp_fname = \"v3.1_oag_publication_14.json\"\n",
    "with open(join(dir_path, dblp_fname), \"r\", encoding=\"utf-8\") as myFile:\n",
    "    for i, line in enumerate(myFile):\n",
    "        if len(line) <= 2:\n",
    "            continue\n",
    "        if i % 10000 == 0: \n",
    "            logger.info(\"reading papers %d\", i)\n",
    "        paper_tmp = json.loads(line.strip())\n",
    "        paper_dict_open[paper_tmp[\"id\"]] = paper_tmp\n",
    "\n",
    "import pandas as pd\n",
    "temp = pd.DataFrame(paper_dict_open)\n",
    "temp = temp.T.reset_index(drop = True)\n",
    "\n",
    "oag_df = temp.copy()\n",
    "\n",
    "s1 = set(oag_df.id)\n",
    "s2 = set(dblp_df.id)\n",
    "d = s1 - s2\n",
    "\n",
    "del temp\n",
    "\n",
    "filtered_oag_df = oag_df[oag_df['id'].isin(d)]\n",
    "dblp_df = pd.concat([dblp_df,filtered_oag_df])\n",
    "dblp_df.to_pickle(dir_path + 'OAG_DBLP-Citation-network.pkl')\n",
    "\n",
    "del oag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bebe6-77d4-40fd-aed5-4c945a584a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa821d-98ee-4ac6-b795-f25c98842b2c",
   "metadata": {},
   "source": [
    "### parse XML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92d605-ef49-4656-88e8-042480412ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_metadata(root, namespaces):\n",
    "\n",
    "    abstract_node = root.find('.//tei:profileDesc/tei:abstract', namespaces=namespaces)\n",
    "    abstract = ' '.join(abstract_node.itertext()) if abstract_node is not None else \"No abstract found\"\n",
    "    return abstract\n",
    "\n",
    "def parse_references(root, namespaces):\n",
    "    references = {}\n",
    "    for bibl in root.xpath('.//tei:biblStruct', namespaces=namespaces):\n",
    "        title = bibl.xpath('.//tei:title[@type=\"main\"]', namespaces=namespaces)\n",
    "        authors = []\n",
    "        for author in bibl.xpath('.//tei:author', namespaces=namespaces):\n",
    "            forename = author.xpath('.//tei:persName/tei:forename/text()', namespaces=namespaces)\n",
    "            surname = author.xpath('.//tei:persName/tei:surname/text()', namespaces=namespaces)\n",
    "            authors.append(' '.join(forename + surname))\n",
    "        authors = ', '.join(authors) \n",
    "        if title:\n",
    "            ref_id = bibl.get('{http://www.w3.org/XML/1998/namespace}id', '')\n",
    "            references[ref_id] = {'title': title[0].text if title[0].text else \"Unknown Title\", 'authors': authors}\n",
    "    return references\n",
    "\n",
    "def extract_citations(root, namespaces, references):\n",
    "    context_data = []\n",
    "    citation_frequency = {}\n",
    "    section_titles = {}\n",
    "\n",
    "    current_section = \"Introduction\"  \n",
    "    for elem in root.iter():\n",
    "        if elem.tag.endswith('head'):\n",
    "            current_section = ''.join(elem.itertext()).strip()  \n",
    "\n",
    "        if elem.tag.endswith('ref') and elem.get('type') == 'bibr':\n",
    "            target = elem.get('target').strip('#') if elem.get('target') else None\n",
    "            if target:\n",
    "                citation_frequency[target] = citation_frequency.get(target, 0) + 1\n",
    "\n",
    "                if target in references:\n",
    "                    citation_info = references[target]\n",
    "                    parent_paragraph = elem.getparent()\n",
    "                    parent_text = ''.join(parent_paragraph.itertext())\n",
    "                    elem_text = ''.join(elem.itertext())\n",
    "                    \n",
    "                    try:\n",
    "                        context_index = parent_text.index(elem_text)\n",
    "                        pre_text = parent_text[max(0, context_index-50):context_index].strip()\n",
    "                        post_text = parent_text[context_index+len(elem_text):context_index+len(elem_text)+50].strip()\n",
    "                    except ValueError:\n",
    "                        pre_text = \"Text not found\"\n",
    "                        post_text = \"Text not found\"\n",
    "\n",
    "                    context_data.append({\n",
    "                        'Citation Number': citation_frequency[target],\n",
    "                        'ref_title': citation_info['title'],\n",
    "                        'ref_authors': citation_info['authors'],\n",
    "                        'Pre Text': pre_text,\n",
    "                        'Post Text': post_text,\n",
    "                        'Section': current_section\n",
    "                    })\n",
    "\n",
    "    return context_data, citation_frequency, section_titles\n",
    "\n",
    "\n",
    "def aggregate_data(xml_list, dir_path):\n",
    "    all_data = []\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "    \n",
    "    for xml_path in tqdm(xml_list, desc=\"Processing XML files\"):\n",
    "        try:\n",
    "            tree = etree.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            abstract = parse_metadata(root, namespaces)\n",
    "            references = parse_references(root, namespaces)\n",
    "            context_data, citation_frequency, section_titles = extract_citations(root, namespaces, references)  # Unpack the returned tuple correctly\n",
    "\n",
    "            for data in context_data:  # Assure 'data' is a dictionary as expected\n",
    "                data.update({'_id': os.path.basename(xml_path[:-4])})\n",
    "                all_data.append(data)\n",
    "\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"Error parsing {xml_path}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "# Define the directory path and get the list of XML files\n",
    "xml_list = glob.glob(os.path.join(dir_path, 'paper-xml', '*.xml'))\n",
    "\n",
    "# Process all XML files and print the aggregated DataFrame\n",
    "df_aggregated = aggregate_data(xml_list, dir_path)\n",
    "print(df_aggregated.head())\n",
    "\n",
    "temp = df_aggregated.groupby(['ref_title','_id']).count().reset_index()[['ref_title','_id','Section']]\n",
    "\n",
    "temp.columns = ['ref_title','_id','citation_cnt']\n",
    "temp = df_aggregated.merge(temp,on = ['ref_title','_id'],how = 'left')\n",
    "\n",
    "temp = temp.groupby(['citation_cnt', 'ref_title', 'ref_authors', 'Pre Text', 'Post Text','Section','_id']).mean().reset_index()\n",
    "\n",
    "df_aggregated = temp.groupby(['_id', 'ref_title']).agg({\n",
    "    'citation_cnt': 'sum',\n",
    "    'Citation Number': 'mean', \n",
    "    'ref_authors': ' '.join,\n",
    "    'Pre Text': ' '.join,\n",
    "    'Post Text': ' '.join,\n",
    "    'Section': ' '.join \n",
    "}).reset_index()\n",
    "\n",
    "df_aggregated.to_pickle(dir_path + 'tempolary_xml_ref.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f3a0f-4fc9-4dc2-ac82-81a25d0ce180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    _id = os.path.basename(xml_path).split('.')[0]\n",
    "\n",
    "    # タイトルを抽出\n",
    "    title = root.find('.//tei:titleStmt/tei:title', namespaces=ns)\n",
    "    if title is not None:\n",
    "        title_text = title.text\n",
    "    else:\n",
    "        title_text = \"No title found\"\n",
    "\n",
    "    abstract = root.find('.//tei:profileDesc/tei:abstract', namespaces=ns)\n",
    "    if abstract is not None:\n",
    "        abstract_text = ' '.join(abstract.itertext())\n",
    "    else:\n",
    "        abstract_text = \"No abstract found\"\n",
    "\n",
    "    body_text = \"\"\n",
    "    body = root.find('.//tei:text/tei:body', namespaces=ns)\n",
    "    if body is not None:\n",
    "        body_text = ' '.join(body.itertext())\n",
    "    else:\n",
    "        body_text = \"No body text found\"\n",
    "\n",
    "    authors = []\n",
    "    for author in root.findall('.//tei:sourceDesc//tei:author', namespaces=ns):\n",
    "        forenames = author.xpath('.//tei:persName/tei:forename/text()', namespaces=ns)\n",
    "        surnames = author.xpath('.//tei:persName/tei:surname/text()', namespaces=ns)\n",
    "        name = ' '.join(forenames + surnames if surnames else forenames)\n",
    "        email = author.xpath('.//tei:email/text()', namespaces=ns)\n",
    "        email_text = email[0] if email else \"No email provided\"\n",
    "        authors.append((name, email_text))\n",
    "\n",
    "    \n",
    "    references = []\n",
    "    max_index = 0 \n",
    "    reference_dict = {} \n",
    "    \n",
    "    for bibl in root.findall('.//tei:listBibl/tei:biblStruct', namespaces=ns):\n",
    "        xml_id = bibl.get(\"{http://www.w3.org/XML/1998/namespace}id\")\n",
    "        if xml_id and xml_id.startswith('bib'):\n",
    "            index = int(xml_id[3:]) - 1 \n",
    "            max_index = max(max_index, index)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        ref_title = bibl.find('.//tei:title[@type=\"main\"]', namespaces=ns)\n",
    "        if ref_title is not None and ref_title.text:\n",
    "            ref_title_text = ref_title.text\n",
    "        else:\n",
    "            ref_title_text = \"No title\"\n",
    "            valid_entry = False\n",
    "    \n",
    "        ref_authors = []\n",
    "        for ref_author in bibl.findall('.//tei:author/tei:persName', namespaces=ns):\n",
    "            ref_forenames = ref_author.xpath('.//tei:forename/text()', namespaces=ns)\n",
    "            ref_surnames = ref_author.xpath('.//tei:surname/text()', namespaces=ns)\n",
    "            if ref_forenames or ref_surnames:\n",
    "                ref_author_name = ' '.join(ref_forenames + ref_surnames if ref_surnames else ref_forenames)\n",
    "                ref_authors.append(ref_author_name)\n",
    "            else:\n",
    "                valid_entry = False\n",
    "    \n",
    "        reference_dict[index] = {'title': ref_title_text, 'authors': ref_authors, 'valid': True}\n",
    "    \n",
    "    references = [{'title': \"No title\", 'authors': [], 'valid': False}] * (max_index + 1) \n",
    "    for index, ref_info in reference_dict.items():\n",
    "        references[index] = ref_info \n",
    "\n",
    "    \n",
    "    # Attempt to extract publication date\n",
    "    date_nodes = root.xpath('.//*[local-name()=\"date\"]')\n",
    "    publication_date = \"No date found\"\n",
    "    for date in date_nodes:\n",
    "        if 'when' in date.attrib:\n",
    "            publication_date = date.attrib['when']\n",
    "            break  # Assumes first matching 'when' attribute is the correct one\n",
    "\n",
    "    # Attempt to extract publication venue\n",
    "    venue_node = root.xpath('.//*[local-name()=\"monogr\"]/*[local-name()=\"title\"]')\n",
    "    venue_text = venue_node[0].text if venue_node else \"No venue found\"\n",
    "\n",
    "\n",
    "    return _id, title_text, abstract_text, body_text, authors, references, publication_date, venue_text\n",
    "\n",
    "\n",
    "xml_list = glob.glob(dir_path + 'paper-xml/*')\n",
    "\n",
    "df_tot = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0,len(xml_list))):\n",
    "    _id, title, abstract, body, authors, references ,publication_date, venue_text = parse_xml(xml_list[i])\n",
    "\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        '_id': [_id],  \n",
    "        'title': [title],\n",
    "        'abstract': [abstract],\n",
    "        'body': [body],\n",
    "        'authors': [authors],  \n",
    "        'references': [references],  \n",
    "        'publication_date': [publication_date],\n",
    "        'venue_text': [venue_text]\n",
    "    })\n",
    "\n",
    "    df_tot = pd.concat([df_tot,temp_df])\n",
    "\n",
    "df_tot = df_tot.reset_index(drop = True)\n",
    "\n",
    "df_tot.to_pickle(dir_path + 'tempolary_xml.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf6290-db63-4f62-b0bb-432553828e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057f2c4-7b2d-4f2a-8633-d712ab999d20",
   "metadata": {},
   "source": [
    "## text matching using OAG datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f4900-5b84-4a4b-9750-e1cf0306073a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_aggregated['ref_title_norm'] = df_aggregated['ref_title'].str.lower().str.replace(r'[\\s\\-_:.]', '', regex=True)\n",
    "df_aggregated['ref_title_norm'] = df_aggregated['ref_title_norm'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df_aggregated['ref_title_norm'] = df_aggregated['ref_title_norm'].str.lower().str.replace(r'[\\s\\-_:.\\',]', '', regex=True)\n",
    "\n",
    "dblp_df = pd.read_pickle(dir_path + 'OAG_DBLP-Citation-network.pkl')\n",
    "\n",
    "dblp_df['title'] = dblp_df['title'].str.lower().str.replace(r'[\\s\\-_:.]', '', regex=True)\n",
    "dblp_df['title'] = dblp_df['title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "dblp_df['title'] = dblp_df['title'].str.lower().str.replace(r'[\\s\\-_:.\\',]', '', regex=True)\n",
    "\n",
    "df = pd.merge(df_aggregated,dblp_df[['id', 'abstract','title','authors', \n",
    "                                     'venue', 'n_citation', 'keywords', 'year']],left_on = ['ref_title_norm'], right_on = ['title'],how = 'left')\n",
    "\n",
    "df = df.rename(columns = {'abstract':'ref_abstract',\n",
    "                            'venue':'ref_venue','id':'ref_id','year':'ref_year',\n",
    "                            'n_citation':'ref_n_citation','keywords':'ref_keywords'})\n",
    "\n",
    "\n",
    "df['ref_authors_list'] = df['ref_authors'].apply(lambda x: [name.strip() for name in x.split(',')] if isinstance(x, str) else x)\n",
    "\n",
    "def extract_names(authors_list):\n",
    "    try:\n",
    "        authors_dict = json.loads(authors_list.replace(\"'\", '\"')) \n",
    "        names = [author['name'] for author in authors_dict if 'name' in author]\n",
    "    except:\n",
    "        names = []\n",
    "    return names\n",
    "\n",
    "df['authors_list'] = df['authors'].apply(extract_names)\n",
    "\n",
    "def calculate_jaccard(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0  \n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def select_most_similar(group):\n",
    "    if len(group) == 1:\n",
    "        return group\n",
    "    max_sim = 0\n",
    "    best_index = group.index[0]\n",
    "    for i, row in group.iterrows():\n",
    "        sim = calculate_jaccard(row['ref_authors_list'], row['authors_list'])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            best_index = i\n",
    "    return group.loc[[best_index]]\n",
    "\n",
    "cnt = df.groupby(['ref_title_norm', '_id']).count()\n",
    "cnt = cnt.loc[cnt['ref_authors'] == 1].reset_index()[['ref_title_norm', '_id']]\n",
    "cnt['r'] = 1\n",
    "df = df.merge(cnt, on = ['ref_title_norm', '_id'], how = 'left')\n",
    "\n",
    "remain_df = df.loc[(df['r']== 1)].copy()\n",
    "df = df.loc[(df['r']!= 1) & (df['ref_title_norm']!= 'notitle')]\n",
    "\n",
    "grouped = df.groupby(['ref_title_norm', '_id'])\n",
    "processed_df = pd.DataFrame()\n",
    "unprocessed_df = pd.DataFrame()\n",
    "\n",
    "for name, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "    if len(group) > 1:\n",
    "        result = select_most_similar(group)\n",
    "        processed_df = pd.concat([processed_df, result])\n",
    "    else:\n",
    "        unprocessed_df = pd.concat([unprocessed_df, group])\n",
    "\n",
    "final_df = pd.concat([processed_df, remain_df]).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b2f34-7efe-4d67-995a-ee27666899eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.merge(final_df,dblp_df[['id','keywords','year','n_citation','venue']],left_on = '_id', right_on = 'id', how = 'left')\n",
    "t.drop(columns = {'ref_authors_list', 'authors_list', 'r','ref_title_norm','title'}).to_pickle(dir_path + 'xml_analysis_out_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bfcb3-ddf2-4d40-abcc-0ebe3545a4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179fa137-1cb4-40dc-99a5-5c8d0b80251d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## creating dataset and hand-crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249d571-f0a8-4471-9c3b-46c2e90bcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_df = pd.read_pickle(dir_path + 'OAG_DBLP-Citation-network.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180cf4d-5a87-479a-93be-621de2c52b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xml_analysis = pd.read_pickle(dir_path + 'xml_analysis_out_v2.pkl')\n",
    "\n",
    "rulebase_label = pd.read_json(dir_path + 'paper_source_gen_by_rule.json')\n",
    "valid_data = pd.read_json(dir_path + 'paper_source_trace_valid_wo_ans.json')\n",
    "train_data = pd.read_json(dir_path + 'paper_source_trace_train_ans.json')\n",
    "test_data = pd.read_json(dir_path + 'paper_source_trace_test_wo_ans.json')\n",
    "\n",
    "\n",
    "train_data['train_or_valid'] = 'train'\n",
    "valid_data['train_or_valid'] = 'valid'\n",
    "test_data['train_or_valid'] = 'test'\n",
    "\n",
    "data = pd.concat([train_data,valid_data,test_data])\n",
    "data = data[['_id','refs_trace','train_or_valid']].copy()\n",
    "\n",
    "xml_analysis = xml_analysis.merge(data, on = '_id', how = 'left')\n",
    "\n",
    "data = pd.concat([train_data,valid_data,test_data])\n",
    "df_exploded = data.explode('references')[['_id','references','refs_trace','train_or_valid']]\n",
    "\n",
    "t1 = xml_analysis[['_id','ref_id']].drop_duplicates()\n",
    "df_exploded.columns = ['_id','ref_id','refs_trace','train_or_valid']\n",
    "\n",
    "t1['flg'] = 1\n",
    "\n",
    "t2 = df_exploded.merge(t1,on = ['_id','ref_id'],how = 'left')\n",
    "t2 = t2.loc[t2['flg'] != 1]\n",
    "\n",
    "t2 = pd.merge(t2,dblp_df[['id', 'abstract','title','authors', \n",
    "                                     'venue', 'n_citation', 'keywords', 'year']],left_on = ['ref_id'], right_on = ['id'],how = 'left')\n",
    "\n",
    "\n",
    "t2 = t2.rename(columns = {'abstract':'ref_abstract','title':'ref_title','authors':'ref_authors',\n",
    "                            'venue':'ref_venue','id':'_ref_id','year':'ref_year',\n",
    "                            'n_citation':'ref_n_citation','keywords':'ref_keywords'})\n",
    "\n",
    "t2 = pd.merge(t2,dblp_df[['id', 'abstract','title','authors', \n",
    "                                     'venue', 'n_citation', 'keywords', 'year']],left_on = ['_id'], right_on = ['id'],how = 'left')\n",
    "\n",
    "t2_adjusted = t2[xml_analysis.columns.intersection(t2.columns)]\n",
    "result = pd.concat([xml_analysis, t2_adjusted], ignore_index=True)\n",
    "\n",
    "xml_analysis = result.copy()\n",
    "\n",
    "xml_analysis['ref_title'] = xml_analysis['ref_title'].fillna('')\n",
    "df['ref_title'] = df['ref_title'].fillna('')\n",
    "\n",
    "xml_analysis['ref_id'] = xml_analysis['ref_id'].fillna('_ref_idIsNaN')\n",
    "xml_analysis['refs_trace'] = xml_analysis['refs_trace'].fillna('refs_traceIsNaN')\n",
    "\n",
    "rulebase_label = pd.read_json(dir_path + 'paper_source_gen_by_rule.json')\n",
    "\n",
    "rulebase_label = rulebase_label.T\n",
    "\n",
    "def left_align_row(row):\n",
    "    filtered = row.dropna().tolist()\n",
    "    return filtered + [np.nan]*(len(row) - len(filtered))\n",
    "\n",
    "df_aligned = rulebase_label.apply(left_align_row, axis=1)\n",
    "\n",
    "df_aligned = pd.DataFrame(df_aligned).reset_index()\n",
    "\n",
    "df_aligned.columns = ['_id', 'annotation_ref']\n",
    "\n",
    "df_aligned['annotation_ref'] = df_aligned['annotation_ref'].apply(lambda x:x[0])\n",
    "\n",
    "xml_analysis = xml_analysis.merge(df_aligned, on = '_id', how = 'left')\n",
    "\n",
    "temp_xml_analysis = xml_analysis[['_id','ref_title','annotation_ref']].dropna().drop_duplicates().reset_index()\n",
    "temp_xml_analysis['levenshtein_distance'] = 999\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "for i in tqdm(range(0,len(temp_xml_analysis))):\n",
    "        d = levenshtein_distance(temp_xml_analysis['ref_title'][i],temp_xml_analysis['annotation_ref'][i])\n",
    "        temp_xml_analysis['levenshtein_distance'][i] = d\n",
    "\n",
    "temp_xml_analysis['target'] = temp_xml_analysis.groupby('annotation_ref')['levenshtein_distance'].transform('min')\n",
    "\n",
    "temp_xml_analysis['target'] = (temp_xml_analysis['levenshtein_distance'] == temp_xml_analysis['target']).astype(int)\n",
    "\n",
    "xml_analysis = xml_analysis.reset_index()\n",
    "\n",
    "\n",
    "temp_xml_analysis2 = xml_analysis.copy()\n",
    "temp_xml_analysis2.drop(list(temp_xml_analysis['index']), inplace = True)\n",
    "\n",
    "#xml_analysis['target'] = xml_analysis.apply(lambda row: 1 if row['ref_id'] in str(row['refs_trace']) else 0, axis=1)\n",
    "temp_xml_analysis2['target'] = temp_xml_analysis2.apply(lambda row: 1 if row['ref_id'] in str(row['refs_trace']) else 0, axis=1)\n",
    "\n",
    "temp_xml_analysis3 = pd.concat([temp_xml_analysis[['index','target']],temp_xml_analysis2[['index','target']]])\n",
    "xml_analysis = xml_analysis.merge(temp_xml_analysis3[['index','target']], on = ['index'], how = 'left')\n",
    "\n",
    "print(len(xml_analysis))\n",
    "xml_analysis = pd.merge(xml_analysis,dblp_df[['id','abstract','title']],left_on=['_id'],right_on=['id'],how = 'left')\n",
    "print(len(xml_analysis))\n",
    "\n",
    "def update_target(group):\n",
    "    if (group['target'] == 1).any():\n",
    "        return group \n",
    "    else:\n",
    "        group['target'] = -1  \n",
    "    return group\n",
    "\n",
    "xml_analysis = xml_analysis.groupby('_id').apply(update_target).reset_index(drop=True)\n",
    "\n",
    "xml_analysis['Pre Text'] = xml_analysis['Pre Text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['Post Text'] = xml_analysis['Post Text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['Pre Text'] = xml_analysis['Pre Text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['Post Text'] = xml_analysis['Post Text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['Section'] = xml_analysis['Section'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['ref_abstract'] = xml_analysis['ref_abstract'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['abstract'] = xml_analysis['abstract'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['ref_title'] = xml_analysis['ref_title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['title'] = xml_analysis['title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['venue'] = xml_analysis['venue'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "xml_analysis['ref_venue'] = xml_analysis['ref_venue'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "xml_analysis.drop(columns = {'index'}).to_pickle(dir_path + 'dataset_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c273c-7b17-4be1-acf9-bfda015e2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caafeca-cfeb-4b45-97ee-453dd37e25e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
